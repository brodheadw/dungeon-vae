# dungeon-vae
**CLIPâ€‘guided tile generation on top of Stable Diffusion**

Generate cohesive, tileâ€‘able terrain (or city) assets from **text prompts**. The pipeline

1. samples 40 candidates with a lightweight Stableâ€‘Diffusion backbone,
2. embeds each image + the prompt with **CLIP ViTâ€‘L/14**,
3. ranks by cosine similarity, and
4. stitches the **topâ€‘k** into a 3â€¯Ã—â€¯3 preview grid.

The repo also contains a scraper + LoRA fineâ€‘tuning script so you can bias
SD toward a style such as *Watabouâ€¯/â€¯Perilousâ€¯Shores*.

![sample grid](docs/sample_grid.png)

---

## Features

| Area             | Details |
|------------------|---------|
| **Textâ€‘toâ€‘tiles**| 40 diffusion samples â†’ CLIP reâ€‘rank â†’ topâ€‘9 grid |
| **Dual workflows**| â€¢ *Sampler*Â â€“ fresh diffusion scenes<br> â€¢ *Dataset*Â â€“ assemble from scraped tiles |
| **LoRA ready**   | Fineâ€‘tune on as few as 200 images (DreamBooth/LoRA) |
| **Runs on Metal**| Works out of the box on AppleÂ Silicon (MPS backend) |

---

## Installation

### 1Â Â·Â Oneâ€‘click Colab

> <https://colab.research.google.com/github/brodheadw/clip-tile-diffusion/blob/main/notebooks/ClipTileGenerator.ipynb>

### 2Â Â·Â Local setup (conda)

```bash
# clone & enter
git clone https://github.com/brodheadw/clip-tile-diffusion.git
cd clip-tile-diffusion

# create env
conda env create -f environment.yml
conda activate tilediff
```

### 3Â Â·Â (Optionally) install bleedingâ€‘edge diffusers
```bash
pip install --upgrade "git+https://github.com/huggingface/diffusers.git"
```

---

## Quick start

### AÂ Â·Â Generate 9â€‘tile grid from scratch
```bash
python demo.py --mode sampler --prompt "lush canyon ruins"
# â†’ output/grid.png
```

### BÂ Â·Â Assemble from your scraped tiles
```bash
python demo.py --mode dataset \
               --prompt "ancient crystal desert oasis" \
               --data_dir data/tiles \
               --out output/oasis_grid.png
```

---

## ðŸ”¬Â OptionalÂ â€“ Fineâ€‘tune a LoRA on Watabou maps

```bash
accelerate launch train_dreambooth_lora.py \
  --pretrained_model_name_or_path "runwayml/stable-diffusion-v1-5" \
  --instance_data_dir data/perilous \
  --instance_prompt "a fantasy hexâ€‘map in the style of Perilous Shores" \
  --resolution 512 \
  --train_batch_size 1 \
  --num_train_epochs 30 \
  --learning_rate 1e-4 \
  --mixed_precision "no" \
  --output_dir lora-perilous-shores
```
> **AppleÂ Silicon notes**
> * Reduce `--train_batch_size` toÂ 1.
> * You can raise the MPS memory cap with `export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` if you still OOM.

---

## Dataset scraping

| Generator                | License                | Export |
|--------------------------|------------------------|---------|
| **PerilousÂ Shores**      | CCÂ BYÂ 3.0 byÂ Watabou   | PNG/SVG |
| **MedievalÂ Fantasy City**| GPLâ€‘3 / CCâ€‘BYâ€‘SAÂ 4.0   | SVGâ†’PNG |

Run the scraper:
```bash
python scrape_perilous.py --maps 100
# collects â†’ data/perilous/perilous_*seed*.png
```

---

## Repo layout
```
clip-tile-diffusion/
â”œâ”€ demo.py               # CLI entrypoint
â”œâ”€ scrape_perilous.py    # Watabou map scraper (Selenium/Puppeteer)
â”œâ”€ train_dreambooth_lora.py
â”œâ”€ src/
â”‚Â Â  â”œâ”€ sampler.py        # SDâ€‘Turbo wrapper
â”‚Â Â  â”œâ”€ ranker.py         # CLIP reâ€‘ranking
â”‚Â Â  â””â”€ stitch.py         # grid composer
â”œâ”€ environment.yml
â””â”€ docs/
    â””â”€ sample_grid.png
```

---

## Roadmap
- Poisson blending for seamless borders
- Automatic elevation + biome masks â†’ Houdini asset
- Unity/Unreal importer
- Gradio web GUI with prompt history & seed browser

---

## License
- **Code**Â â€“ MIT
- **Generated assets**Â â€“ follow Watabouâ€™s CCÂ BYÂ 3.0 (credit *@watabou*)

---

## Acknowledgements
- **Watabou** â€“ ProcgenÂ Arcana generators
- **StabilityÂ AI** â€“ `sdâ€‘turbo`
- **OpenAI** â€“ CLIP
- **HuggingFace** â€“ diffusers & Accelerate